{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import zipfile\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Conv2D, Conv2DTranspose, Dropout, LeakyReLU, Activation, ZeroPadding2D\n",
    "from keras.models import Model, Sequential\n",
    "import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 5 6 1 7 8 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # type: ignore\n",
    "\n",
    "# example caption\n",
    "caption = \"A red sports car parked on a city street.\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# define tokenizer and fit on captions\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([caption])\n",
    "\n",
    "# convert caption into numerical array\n",
    "sequences = tokenizer.texts_to_sequences([caption])\n",
    "\n",
    "# define embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "\n",
    "# define numerical array\n",
    "max_length = 10\n",
    "padded_seq = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# print numerical array\n",
    "print(padded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anisn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anisn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(img_path):\n",
    "    # Load the pre-trained CNN model\n",
    "    model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(img_path, target_size=(224, 224)) # type: ignore\n",
    "    img = image.img_to_array(img) # type: ignore\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "\n",
    "    # Extract features from the image\n",
    "    features = model.predict(img)\n",
    "    features = np.reshape(features, (features.shape[0], -1))\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers\n",
    "from SAE_model import *\n",
    "\n",
    "\n",
    "\n",
    "# # Define the autoencoder model\n",
    "# autoencoder = Model(encoder_inputs, decoder_outputs)\n",
    "\n",
    "# # Compile the model\n",
    "# autoencoder.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def encode_decode(caption_str, autoencoder):\n",
    "    # Convert the caption to a tensor\n",
    "    caption_tensor = torch.tensor(np.array([extract_features(caption_str)]))\n",
    "\n",
    "    # Pass the caption through the autoencoder\n",
    "    encoded = autoencoder.encoder(caption_tensor)\n",
    "    decoded = autoencoder.decoder(encoded)\n",
    "    vocab_idx2word = {i: w for w, i in decoded[0]}\n",
    "\n",
    "    # Convert the decoded tensor back to a string\n",
    "    caption = ' '.join([vocab_idx2word[i] for i in decoded[0]])\n",
    "\n",
    "\n",
    "\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def extract_important_words(caption):\n",
    "    # Tokenize the caption into words\n",
    "    words = word_tokenize(caption)\n",
    "\n",
    "    # Define the set of stop words to remove\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "    # Remove stop words and non-alphabetic characters\n",
    "    important_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "\n",
    "    return important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the zip file containing the input files\n",
    "with zipfile.ZipFile(\"data/train2017.zip\", \"r\") as zip_ref:\n",
    "    \n",
    "    for file_name in zip_ref.namelist():\n",
    "        \n",
    "        if file_name.endswith(\".txt\"):\n",
    "            # Read the contents of the file\n",
    "            with zip_ref.open(file_name) as file:\n",
    "                caption = file.read().decode('utf-8')\n",
    "                caption = encode_decode(caption,autoencoder)\n",
    "            # call the imp function\n",
    "            important_words = extract_important_words(caption)\n",
    "            # Create a dictionary \n",
    "            data = {\n",
    "                \"file_name\": file_name,\n",
    "                \"important_words\": important_words\n",
    "            }\n",
    "            # Convert the dictionary to JSON format\n",
    "            json_data = json.dumps(data)\n",
    "\n",
    "            \n",
    "            json_file_name = os.path.splitext(file_name)[0] + \".json\"\n",
    "            # save the JSON file\n",
    "            with zip_ref.open(json_file_name, \"w\") as json_file:\n",
    "                json_file.write(json_data.encode('utf-8'))\n",
    "\n",
    "# Add the JSON files to the zip folder\n",
    "with zipfile.ZipFile(\"data/train2017.zip\", \"a\") as zip_ref:\n",
    "    # Loop through all the JSON files in the directory\n",
    "    for file_name in os.listdir():\n",
    "        if file_name.endswith(\".json\"):\n",
    "            # Add the JSON file to the zip folder\n",
    "            zip_ref.write(file_name)\n",
    "            # Remove the JSON file from the directory to avoid redundancy\n",
    "            os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"C:/Users/Anisn/Documents/Final Project/data/train2017/images\"\n",
    "\n",
    "images = os.listdir(folder_path)\n",
    "\n",
    "print(images[:100])\n",
    "\n",
    "path = \"C:/Users/Anisn/Documents/Final Project/data/train2017/captions\"\n",
    "file_names = os.listdir(path)\n",
    "\n",
    "captions = []\n",
    "for file_name in file_names[:100]:\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        with open(os.path.join(path, file_name), \"r\") as file:\n",
    "            text = file.read()\n",
    "            captions.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_words = []\n",
    "for i in captions:\n",
    "    important_words.append(extract_important_words(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_important_words(caption):\n",
    "    # Tokenize the caption into words\n",
    "    words = word_tokenize(caption)\n",
    "\n",
    "    # Define the set of stop words to remove\n",
    "    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "\n",
    "    # Remove stop words and non-alphabetic characters\n",
    "    important_words = [word.lower() for word in words if word.lower() not in stop_words and word.isalpha()]\n",
    "\n",
    "    return important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def process_caption(caption):\n",
    "    doc = nlp(caption)\n",
    "    important_words = []\n",
    "    for token in doc:\n",
    "        if token.pos_ in [\"NOUN\", \"VERB\", \"ADJ\"]:  # extract only NOUN, VERB, ADJ\n",
    "            important_words.append(token.text.lower())\n",
    "    # convert words to numerical values\n",
    "    word_to_num = {'car': 1, 'truck': 2, 'motorbike': 3, 'bus': 4, 'vehicle': 5, 'street': 6, 'road': 7, 'city': 8, 'forest': 9, 'tree': 10, 'suv': 11, 'house': 12, 'lawn': 13, 'flower': 14, 'skyline': 15, 'school': 16, 'neighborhood': 17, 'tractor': 18, 'hay': 19, 'baler': 20, 'field': 21, 'wheat': 22, 'sun': 23, 'hill': 24, 'convertible': 25, 'coastal': 26, 'ocean': 27, 'view': 28, 'palm': 29, 'delivery': 30, 'warehouse': 31, 'worker': 32, 'cargo': 33, 'box': 34, 'pallet': 35, 'rv': 36, 'campground': 37, 'forest': 38, 'lake': 39, 'bird': 40, 'chirping': 41, 'motorcycle': 42, 'people': 43, 'traffic': 44, 'bustling': 45}\n",
    "    num_array = [word_to_num[word] for word in important_words if word in word_to_num]\n",
    "    return num_array\n",
    "processed_captions = [process_caption(caption) for caption in captions]\n",
    "\n",
    "print(processed_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "import_words = []\n",
    "\n",
    "for i in captions:\n",
    "  import_words.append(extract_important_words(i))\n",
    "\n",
    "# print(import_words)\n",
    "data = []\n",
    "i = [i for i in range(100)]\n",
    "for a,b,c,d,e in zip(images,captions, i, import_words, processed_captions):\n",
    "  data.append({\"IMG\" : a,\"caption\": b,\"idx\": c ,\"IMP\": d, \"values\": e})\n",
    "\n",
    "# Save data to JSON file\n",
    "with open(\"saved_data/captions_info.json\", \"w\") as outfile:\n",
    "    json.dump(data, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
